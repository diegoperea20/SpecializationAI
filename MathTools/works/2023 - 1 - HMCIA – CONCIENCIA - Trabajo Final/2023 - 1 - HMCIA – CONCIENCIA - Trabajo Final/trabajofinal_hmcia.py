# -*- coding: utf-8 -*-
"""TRABAJOFINAL_HMCIA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_o8Tawfi4B3Y3FfQ1d9t7sONl8KTmhTl

## **Trabajo final de Herramientas matematicas y computacionales para I.A.**

### **Team conciencIA**

Samir Hassan - 2190041

Gabriel Jeannot - 2185887

Carlos Osorio - 2230894

Luis Pareja - 2185833

Diego Perea - 2185751

# Punto 1

1.    Para el primer punto, una vez leido el articulo y buscado en internet. Se ha determinado por escoger un data set de kaggle.com, el cual cumple con un patron lineal de los datos expuestos de manera multivariable. Este data set, esta hecho para predecir las ventas de compañías en función del presupuesto de marketing invertido en redes sociales (YouTube, Facebook y periódico), por lo cuál, a manera de puesta en práctica y demostración, es un grupo de datos muy interesante para probar la regresion lineal multivariable.

Por favor seleccionar de la carpeta que se adjunto en el parcial, el data set llamado 'Marketing_Data.csv' para poder ejercutar el primer punto, debido a que este data set se almaceno localmente.
"""

import numpy as np #Se importa la biblioteca de numpy
import matplotlib.pyplot as plt #Se importa la biblioteca de matplotlib
import pandas as pd #Se importa la biblioteca de pandas
from google.colab import drive #
drive.mount('/content/drive/') 
print("Por favor seleccionar de la carpeta que se adjunto en el parcial, el data set llamado 'Marketing_Data.csv' para poder ejercutar el primer punto, debido a que este data set se almaceno localmente.")
from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
  print(f"Archivo {filename} cargado exitosamente")

mu = []
std = []

def plot_data(x, y): #Se define una funcion para graficar los datos y evaluar la viabilidad de la RL en este caso.
	plt.xlabel('y') #Se nombra eje x
	plt.ylabel('Sales') #Se nombra el eje Y
	plt.plot(x[:,0], y, 'bo', label='Youtube') #Se realiza la grafica solo en función de los datos de la columna Youtube
	plt.plot(x[:,1], y, 'ro', label='Facebook')
	plt.plot(x[:,2], y, 'go', label='Newspaper')
	plt.show() #Se muestra la gráfica

def normalize(data): #Se procede a crear la función de normalizacion posteriomente llamada.
	for i in range(0,data.shape[1]-1): #Se crea un ciclo para recorrer el conjunto de los datos.
		data[:,i] = ((data[:,i] - np.mean(data[:,i]))/np.std(data[:, i])) #Z = (X - μ) / σ, donde μ = media y σ= desviacion estandar.
		mu.append(np.mean(data[:,i]))
		std.append(np.std(data[:, i]))

#filename= "/content/drive/MyDrive/2023_1_HMCIA_conciencIA_TrabajoFinal/Marketing_Data.csv"
def load_data(filename): #Se crea una función para cargar el data-set
	df = pd.read_csv(filename, sep=",", index_col=False) #Se lee el archivo donde se encuentran los datos
	df.columns = ["youtube", "facebook", "newspaper", "sales"] #Se definen las columnas con las variables del data set
	data = np.array(df, dtype=float) #Se crea un arreglo del conjunto de datos
	plot_data(data[:,:3], data[:, -1]) #Se usa para graficar una RL multivariante y su etiqueta (Y)
	normalize(data) #Se normalizan sus valores para un correcto entrenamiento del modelo y evitar desequilibrios.
  #Comentario: Normalizacion de caracteristicas: Z = (X - μ) / σ, donde μ = media y σ= desviacion estandar.
	return data[:,:3], data[:, -1] #Devuelve X (varaibles para predecir) y Y (su etiqueta o salida).

load_data(filename)

"""Comentario: Como se pudo observar en la grafica realizada anteriormente, los datos cumplen con un patron de crecimiento lineal o en pendiente, y aunque muestra una mayor dispersion de estos a medida que aumentan las entradas (X), sigue con la misma tendencia de crecimiento la cual es posible ajustar aproximadamente, trazando una linea.

Comentario: Para el desarrollo de la hipotesis, segun el data set de tres caracteristicas, es el siguiente: hθ ( x ) = θ0 + θ1x1 + θ2x2 + 03x3. Esto se pondra, segun el articulo, en una funcion de python que regresa la hipotesis.
"""

def h(x,theta): #Funcion de hipotesis de RL.
	return np.matmul(x, theta) #Funcion de numpy multiplicacion matricial.

"""Comnentario: Se realiza la función de costo descrita en el articulo, la cual evaluara la calidad del modelo."""

def cost_function(x, y, theta): #Se define el nombre de la funcion de costo
	return ((h(x, theta)-y).T@(h(x, theta)-y))/(2*y.shape[0]) #Regresa el costo del modelo de RL.

"""Comentario: Se realiza la funcion de gradiente de descenso usada en el articulo solicitado, el cuál tendra como función ajustar los parámetros para minimizar la función de costo"""

def gradient_descent(x, y, theta, learning_rate=0.1, num_epochs=10): #Se define la funcion de gradiente de descenso
	m = x.shape[0]
	J_all = []
	
	for _ in range(num_epochs): #Se realiza un ciclo en funcion del numero de epocas.
		h_x = h(x, theta)
		cost_ = (1/m)*(x.T@(h_x - y)) #Funcion de costo
		theta = theta - (learning_rate)*cost_ #Vector de parametros de 0s para la hipotesis
		J_all.append(cost_function(x, y, theta))

	return theta, J_all #retorna los valores de theta (vector de parametros) y J_all (lista que contiene función de costo despues de cada epoca)

mu = []
std = []

def test(theta, x): #Se define la función de test, con el fin de poner a prueba el modelo y predecir algunos valores reales (inventados por nosotros).
  x[0] = (x[0] - mu[0])/std[0] #Variable independiente 'youtube'.
  x[1] = (x[1] - mu[1])/std[1] #Variable independiente 'facebook'.
  x[2] = (x[2] - mu[2])/std[2] #Variable independiente 'periodico'.
	
  y= theta[0] + theta[1]*x[0] + theta[2]*x[1] + theta[3]*x[2] #Le da el valor a predecir de la cantidad de ventas en una compañia, a partir de las variables independientes 'x'.
  print("Company sales: ", y)

def plot_cost(J_all, num_epochs): #Se define una función para plotear la grafica de costo
	plt.xlabel('Epochs') #Se nombra el eje x
	plt.ylabel('Cost') #Se nombra el eje y
	plt.plot(num_epochs, J_all, 'm', linewidth = "3") #Se configura la grafica
	plt.show() #Se muestra la grafica

#filename= "/content/drive/MyDrive/2023_1_HMCIA_conciencIA_TrabajoFinal/Marketing_Data.csv" #Se define la ruta del datas set
x,y = load_data(filename) #Se llama la funcion load_data para cargar en x y y los valores del dataset.
y = np.reshape(y, (171,1)) #Se aplica un reshape al conjunto de datos.
x = np.hstack((np.ones((x.shape[0],1)), x)) #Agrega una columna adicional de unos (1) 
theta = np.zeros((x.shape[1], 1)) #Inicializa el vector de theta con ceros.
learning_rate = 0.1 #Se especifica la tasa de aprendizaje del modelo
num_epochs = 50 #Se define el numero de epocas para el entrenamiento
theta, J_all = gradient_descent(x, y, theta, learning_rate, num_epochs) #Se llama a la funcion de descenso de gradiente que devuelve un historial de todas las funcionesde costo y el vector final de parametros theta
J = cost_function(x, y, theta) #Define el valor de la funcion de costo
print("Cost: ", J) #Imprime el valor de J
print("Parameters: ", theta) #Imprime los valores de theta (vector)


#Comentario: A modo de evaluar el cambio en la funcion de costo de acuerdo a las epocas de entrenamiento del modelo, se realiza un grafico que plotea la vairable J_all, la cual se define como el historial de de todos esos
#resultados de la función de costo. Por tanto, con este grafico, se puede observar como dichos valores en la función de costo disminuyen progresivamente en cada epoca.

#for testing and plotting cost 
n_epochs = [] #Se define un arreglo para el numero de epocas del modelo RL.
jplot = [] #Se define un arreglo para los valores de la funcion de costo en cada epoca.
count = 0 #Contador del ciclo
for i in J_all: #Se define el ciclo for
	jplot.append(i[0][0]) #Se inicia la grafica en Y.
	n_epochs.append(count) #Se inicia la grafica en X
	count += 1 #Contador +1
jplot = np.array(jplot) #Se toma el arreglo de jplot
n_epochs = np.array(n_epochs) #Se toma el arreglo de epocas
plot_cost(jplot, n_epochs) #Se grafica estos dos arreglos

# Comentario: Con esto ya definido, nuestro modelo de RL esta listo para predecir el numero de ventas de una compañia en función de su estrategia de marketing plasmada en la inversión en medios publicitarios.

#Comentario: Ahora se procedera a probar el codigo llamando a una función de prueba que tomará como entrada la inversión en medios publicitarios: youtube, facebook y periodico. Y el vector theta devuelto por el modelo
#de RL y que nos dara la cantidad de ventas de cada compañia.

print(mu)
test(theta, [124, 12, 30])

"""Comentario: Una vez evaluado el modelo, se tiene que la función de costo que arroja el modelo es de 1.97, por lo cual podriamos decir que el error cuadratico medio de los valores del data set, entre sus reales y estimados, es de + ó - 1.97.

## Ahora, se procederá a realizar regresion lineal, usando la librería SKLEARN para comparar los dos modelos y poder tener una idea de la precision de sus algoritmos.
"""

#Se importan los paquetes necesaios de la libreria sklearn
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn import preprocessing

#filename= "/content/drive/MyDrive/2023_1_HMCIA_conciencIA_TrabajoFinal/Marketing_Data.csv" #Nuevamente se llama la ruta del archivo csv del data set en una variable llamada filename.
df = pd.read_csv(filename) #Se importa y lee el data set con la funcion pf.read
#df.drop(columns= 0, inplace = True,axis=1)
  
print(df.head()) #Se imprimen las 5 primeras filas de datos.
print(df.columns) #Se imprimen las columnas de datos.

#Se crea una trama dispersa para visualizar la relación entre la variable independiente 'youtube' y 'sales'
sns.scatterplot(x='youtube',
                y='sales', data=df)

#Se procede a agrupar las caracteristicas en variables, dado que 'x' contiene variables independientes y 'y' una variable dependiente.

X = df.drop('sales',axis= 1) #Agrupa las variables independientes
y = df['sales'] #Agrupa las variables dependientes
print(X) #Se imprime para observar el resultado
print(y) #""

#Se crea un conjunto de datos para entrenamiento y pruebas del modelo, y se ajusta el tamaño de la prueba al 30% de los datos. Por tanto el conjunto de datos de
#entrenamiento queda al 70%.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=101)

#Se procede a crear un modelo de regresion lineal
model = LinearRegression() #con este metodo se cre un modelo de RL simple, y este se importa de la clase sklearn.linear_model

#Se procede a ajustar el modelo con los datos de entrenamiento anteriormente configurados.
model.fit(X_train,y_train)

#Se realizan predicciones sobre el conjunto de datos de prueba
predictions = model.predict(X_test)

#Se evalua el modelo con metricas de error absoluto medio y error cuadratico medio. Cabe resaltar que entre menor de el numero, mejor sera el rendimiento del modelo.
print(
  'mean_squared_error : ', mean_squared_error(y_test, predictions))
print(
  'mean_absolute_error : ', mean_absolute_error(y_test, predictions))

#Comentario: Si se desea ignorar los valores anormales del dataset, MAE es una mejor opción, pero si se quiere incluirlos en una función de perdida, RMSE es la mejor opción.

"""Con el entrenamiento de un modelo bajo el uso de funciones de RL de la libreria SKLEARN, se obtiene que, bajo el entrenamiento de un modelo con un dataset repartido a: 70% de datos de 'train' y 30% de datos de 'test', el valor del error cuadratico medio en este caso es de 4.562, generando menor precisión que el modelo de RL entrenado inicialmente.

## Comparación entre los dos modelos:

En general, un valor de función de costo más bajo indica un mejor ajuste del modelo a los datos de entrenamiento. Sin embargo, la elección del mejor valor dependerá del contexto y de los objetivos del análisis.

En este caso, dado que la función de costo es más baja que el error cuadrático medio, podríamos decir que el ajuste del modelo es mejor en términos de la función de costo. Sin embargo, es importante tener en cuenta que la comparación de ambos valores puede ser más relevante dentro de un mismo contexto y con un conjunto de datos específico, por lo que es importante evaluar y comparar los resultados en relación a los objetivos y el contexto del análisis.

## Creditos y referencias del primer punto:

1. https://medium.com/analytics-vidhya/basics-and-beyond-linear-regression-c12d99a4df35
2. https://www.geeksforgeeks.org/multiple-linear-regression-with-scikit-learn/
3. https://www.geeksforgeeks.org/python-linear-regression-using-sklearn/

# Punto 3 - Punto 1 Con Optimizadores

## Optimizadores

La variable X_train  contiene un conjunto de datos de entrenamiento, y se utiliza el método shape para obtener la forma de la matriz. La propiedad shape devuelve una tupla que contiene las dimensiones de la matriz, y se accede al número de columnas de la matriz de entrenamiento utilizando el índice 1 en la tupla.

El valor resultante de input_dim_point1 es el número de columnas de la matriz de entrenamiento, lo que se usa como entrada para el modelo de aprendizaje automático.

La variable num_clases_point1 se inicializa con un valor de 1, lo que sugiere que este modelo para  estar diseñado para realizar una tarea de regresión.
"""

input_dim_point1 = X_train.shape[1]
num_clases_point1 = 1
print(input_dim_point1)

"""Importar librerias necesarias para creacion del modelo con optimizadores """

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import plot_model

"""La función model_Nadam_point1 crea un modelo secuencial y agrega tres capas densas a la red neuronal. La primera capa densa tiene 10 neuronas y utiliza la función de activación 'relu'. La segunda capa densa también tiene 10 neuronas y utiliza la misma función de activación. La última capa densa tiene un número de neuronas igual al número de clases definido en num_clases_point1, que como se mencionó en la pregunta anterior, es 1. Esta capa utiliza una función de activación lineal.

El método summary() se utiliza para imprimir un resumen de la arquitectura del modelo.

El optimizador utilizado en este modelo es el algoritmo de optimización Nadam, que se inicializa con un learning_rate de 0.001, beta_1 de 0.9, beta_2 de 0.999 y epsilon de 1e-07. Este optimizador es utilizado para minimizar la función de pérdida definida como 'mse', que corresponde a la función de error cuadrático medio.
La función de pérdida se utiliza para evaluar el rendimiento del modelo durante el entrenamiento y la fase de prueba. Además, se especifica la métrica de precisión o accuracy para evaluar el rendimiento del modelo durante el entrenamiento y la fase de prueba.
"""

def model_Nadam_point1():
  model = Sequential()
  model.add(Dense(10, input_dim = input_dim_point1, activation='relu'))
  model.add(Dense(10, activation='relu'))
  model.add(Dense(num_clases_point1, activation='linear'))

  model.summary()

  opt = tf.keras.optimizers.Nadam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name="Nadam"
)
 
  model.compile(loss = 'mse', optimizer = opt ,metrics=['accuracy'])

  return model

"""La función model_Adam_point1 crea un modelo secuencial y agrega tres capas densas a la red neuronal, con la misma configuración que el modelo anterior. La primera capa densa tiene 10 neuronas y utiliza la función de activación 'relu'. La segunda capa densa también tiene 10 neuronas y utiliza la misma función de activación. La última capa densa tiene un número de neuronas igual al número de clases definido en num_clases_point1.
El método summary() se utiliza para imprimir un resumen de la arquitectura del modelo.

En este modelo, el optimizador utilizado es el algoritmo de optimización Adam, que se inicializa con un learning_rate de 0.1. El learning rate es un parámetro que controla la velocidad de aprendizaje del modelo y puede ser ajustado para obtener un mejor rendimiento.
"""

def model_Adam_point1():
  model = Sequential()
  model.add(Dense(10, input_dim = input_dim_point1, activation='relu'))
  model.add(Dense(10, activation='relu'))
  model.add(Dense(num_clases_point1, activation='linear'))

  model.summary()

  opt = tf.keras.optimizers.Adam(learning_rate=0.1)
 
  model.compile(loss = 'mse', optimizer = opt, metrics=['accuracy'])

  return model

"""La función model_Adamax_point1 crea un modelo secuencial y agrega dos capas densas a la red neuronal. La primera capa densa tiene 10 neuronas y utiliza la función de activación 'relu'. La última capa densa tiene un número de neuronas igual al número de clases definido en num_clases_point1, que como se mencionó anteriormente, es 1. Esta capa utiliza una función de activación lineal.

El método summary() se utiliza para imprimir un resumen de la arquitectura del modelo.

En este modelo, el optimizador utilizado es el algoritmo de optimización Adamax, que se inicializa con un learning_rate de 0.001, beta_1 de 0.9, beta_2 de 0.999 y epsilon de 1e-07. Este optimizador es utilizado para minimizar la función de pérdida definida como 'mse', que corresponde a la función de error cuadrático medio
"""

def model_Adamax_point1():
  model = Sequential()
  model.add(Dense(10, input_dim = input_dim_point1, activation='relu'))
  model.add(Dense(num_clases_point1, activation='linear'))

  model.summary()

  opt =tf.keras.optimizers.Adamax(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name="Adamax"
)
 
  model.compile(loss = 'mse', optimizer = opt, metrics=['accuracy'])

  return model

"""Crea una instancia del modelo definido por la función model_Adam_point1() y lo asigna a la variable modelA1_Adam_point1."""

modelA1_Adam_point1 = model_Adam_point1()

modelA2_Nadam_point1 = model_Nadam_point1()

modelA2_Adamax_point1 = model_Adamax_point1()

"""Se ajusta el modelo modelA1_Adam_point1 a los datos de entrenamiento X_train e y_train, utilizando el optimizador Adam y la función de pérdida MSE, durante 250 épocas. El tamaño de lote utilizado en cada iteración del entrenamiento es de 100 observaciones.

Además, se utiliza el conjunto de datos de validación (X_test, y_test) para evaluar el modelo después de cada época. La evaluación se realiza utilizando la misma función de pérdida y métrica de precisión que se especificó en la compilación del modelo.

El resultado de esta operación se almacena en la variable history1_point1, que contiene información sobre la evolución del error de entrenamiento y validación a lo largo de las épocas
"""

history1_point1 = modelA1_Adam_point1.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=250, batch_size=100, verbose=0)

history2_point1 = modelA2_Nadam_point1.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=250, batch_size=100, verbose=0)

history3_point1 = modelA2_Adamax_point1.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=250, batch_size=100, verbose=0)

"""Se importa la función mean_squared_error de la librería sklearn.metrics. Esta función se utiliza para calcular el error cuadrático medio (MSE) entre dos conjuntos de datos.

A continuación, se utilizan los modelos modelA1_Adam_point1, modelA2_Nadam_point1 y modelA2_Adamax_point1 previamente definidos para hacer predicciones sobre el conjunto de datos de prueba X_test. Las predicciones se almacenan en las variables y_pred_point1_Adam, y_pred_point1_Nadam y y_pred_point1_Adamax, respectivamente.

Finalmente, se calcula el MSE entre las etiquetas verdaderas y_test y las predicciones realizadas por cada modelo. Estos resultados se almacenan en las variables mse_adam_point1, mse_Nadam_point1 y mse_adamax_point1, y se imprimen en la pantalla 
"""

from sklearn.metrics import mean_squared_error

# Predecir con el modelo
y_pred_point1_Adam = modelA1_Adam_point1.predict(X_test)
y_pred_point1_Nadam = modelA2_Nadam_point1.predict(X_test)
y_pred_point1_Adamax = modelA2_Adamax_point1.predict(X_test)
# Calcular el MSE
mse_adam_point1 = mean_squared_error(y_test, y_pred_point1_Adam)
mse_Nadam_point1 = mean_squared_error(y_test, y_pred_point1_Nadam)
mse_adamax_point1 = mean_squared_error(y_test, y_pred_point1_Adamax)

print('Mean Squared Error Adam punto 1  :', mse_adam_point1)
print('Mean Squared Error Nadam punto 1  :', mse_Nadam_point1)
print('Mean Squared Error Adamax punto 1  :', mse_adamax_point1)

"""## Modelo de regresion"""

#Se procede a crear un modelo de regresion lineal
model = LinearRegression() #con este metodo se cre un modelo de RL simple, y este se importa de la clase sklearn.linear_model

#Se procede a ajustar el modelo con los datos de entrenamiento anteriormente configurados.
model.fit(X_train,y_train)

#Se realizan predicciones sobre el conjunto de datos de prueba
predictions = model.predict(X_test)

#Se evalua el modelo con metricas de error absoluto medio y error cuadratico medio. Cabe resaltar que entre menor de el numero, mejor sera el rendimiento del modelo.
print(
  'mean_squared_error : ', mean_squared_error(y_test, predictions))
print(
  'mean_absolute_error : ', mean_absolute_error(y_test, predictions))

#Comentario: Si se desea ignorar los valores anormales del dataset, MAE es una mejor opción, pero si se quiere incluirlos en una función de perdida, RMSE es la mejor opción.

"""Con el entrenamiento de un modelo bajo el uso de funciones de RL de la libreria SKLEARN, se obtiene que, bajo el entrenamiento de un modelo con un dataset repartido a: 70% de datos de 'train' y 30% de datos de 'test', el valor del error cuadratico medio en este caso es de 4.562, generando menor precisión que el modelo de RL entrenado inicialmente.

## Comparación entre los dos modelos:
En general, un valor de función de costo más bajo indica un mejor ajuste del modelo a los datos de entrenamiento. Sin embargo, la elección del mejor valor dependerá del contexto y de los objetivos del análisis.

En este caso, dado que la función de costo es más baja que el error cuadrático medio, podríamos decir que el ajuste del modelo es mejor en términos de la función de costo. Sin embargo, es importante tener en cuenta que la comparación de ambos valores puede ser más relevante dentro de un mismo contexto y con un conjunto de datos específico, por lo que es importante evaluar y comparar los resultados en relación a los objetivos y el contexto del análisis.

## Creditos y referencias del primer punto:

1. https://medium.com/analytics-vidhya/basics-and-beyond-linear-regression-c12d99a4df35
2. https://www.geeksforgeeks.org/multiple-linear-regression-with-scikit-learn/
3. https://www.geeksforgeeks.org/python-linear-regression-using-sklearn/

# Punto 2

## Dataset Utilizado

Se decidió crear un modelo y entrenarlo con el dataset de cáncer cervical ([dataset](https://archive.ics.uci.edu/ml/datasets/Cervical+Cancer+Behavior+Risk)), con el modelo se podrá predecir si una mujer padece o no cáncer de cuello uterino basandose en ciertas caracteristicas (hábitos alimenticios, higiene, motivaciones, etc).

Poder predecir o diagnosticar enfermedades tan peligrosas como lo pueden ser el cáncer es una ayuda muy valiosa para los pacientes que las padecen ya que pueden empezar el tratamiento correspondiente lo mas pronto posible y de esta manera salvar muchas vidas.

La salida de este modelo es un valor booleano que indica si una paciente tiene o no cáncer de cuello uterino, esto hace que cuente como un problema de clasificación binaria, este tipo de problemas se trabajan idealmente implementando un modelo con regresion logistica lineal que permite evaluar la probabilidad que tiene cada mujer de tener o no cancer de cuello uterino, según las caracteristicas antes mencionadas de las mujeres encuestadas.

Este dataset cuenta con 18 atributos más una salida (clase):

1. behavior_eating
2. behavior_personalHygine
3. intention_aggregation
4. intention_commitment
5. attitude_consistency
6. attitude_spontaneity
7. norm_significantPerson
8. norm_fulfillment
9. perception_vulnerability
10. perception_severity
11. motivation_strength
12. motivation_willingness
13. socialSupport_emotionality
14. socialSupport_appreciation
15. socialSupport_instrumental
16. empowerment_knowledge
17. empowerment_abilities
18. empowerment_desires
19. ca_cervix (this is class attribute, 1=has cervical cancer, 0=no cervical cancer)
"""

import pandas as pd
from pandas import read_csv
# load the dataset
url='https://archive.ics.uci.edu/ml/machine-learning-databases/00537/sobar-72.csv'

df = read_csv(url, sep=",", engine='python', na_values="?")

# summarize the dataset
print(df.describe())

"""Visualizamos el dataset"""

df.head()

"""Revisamos los dataypes de cada columna"""

print(df.dtypes)

"""Buscamos datos nulos en el dataset"""

df.isnull().sum()

"""Ahora separamos el dataset en entradas (x)y salida (y), luego se divide en sus respectivas variables de entrenamiento y test. Como es habitual eñ 80% del dataset va a pertenecer al entrenamiento y el resto (20%) al test."""

from sklearn.model_selection import train_test_split

y = df['ca_cervix']
x = df.drop(['ca_cervix'], axis = 1)

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42)

"""## Código

En la implementación del código se inicia con la creación de la clase CustomLogisticRegression, donde se definen los métodos a usar. En primer lugar se entrena con el uso de la gradiente descendiente, tomando en cuenta la perdida de cada epoca y la taza de aprendizaje, actualizando en cada entrenamiento los pesos y el bias del modelo y aproximando una predicción menor o igual a 0,5 a 0 y a 1 en caso contrario, y guardando los datos de las precisiones de entrenamiento y testeo.
Para el cálculo de las gradientes, se deriva la pérdida, siendo esta el real valor del error.
Para la actualización de los pesos, se tiene en cuanta el error y el learning rate, restandole al valor actual del parametro, el producto del error por el learning rate.
Para las predicciones, se utiliza el método predict, el cual se encarga de multiplicar las entradas por sus pesos y sumarle el bias, e ingresar el resultado en la ecuación sigmoide.
Por último, se definen la ecuación sigmoide, y dos métodos para transformar los valores de 'x' y 'y' según se necesitan en el resto de métodos.
"""

import copy
import numpy as np
from sklearn.metrics import accuracy_score

class CustomLogisticRegression():
    def __init__(self):
        self.losses = []
        self.train_accuracies = []
        self.test_accuracies = []

    def fitGradDesc(self, x, y, x_t, y_t, epochs):
        x = self._transform_x(x)
        y = self._transform_y(y)
        
        x_t = self._transform_x(x_t)
        y_t = self._transform_y(y_t)

        self.weights = np.zeros(x.shape[1])
        self.bias = 0

        for i in range(epochs):
            x_dot_weights = np.matmul(self.weights, x.transpose()) + self.bias
            x_dot_weights_t = np.matmul(self.weights, x_t.transpose()) + self.bias

            pred = self._sigmoid(x_dot_weights)
            pred_t = self._sigmoid(x_dot_weights_t)

            loss = self.compute_loss(y, pred)
            error_w, error_b = self.compute_gradients(x, y, pred)
            self.update_model_parameters(error_w, error_b)

            pred_to_class = [1 if p > 0.5 else 0 for p in pred]
            pred_to_class_t = [1 if p > 0.5 else 0 for p in pred_t]

            self.train_accuracies.append(accuracy_score(y, pred_to_class))
            self.test_accuracies.append(accuracy_score(y_t, pred_to_class_t))

            self.losses.append(loss)


    def compute_loss(self, y_true, y_pred):
        # binary cross entropy
        y_zero_loss = y_true * np.log(y_pred + 1e-9)
        y_one_loss = (1-y_true) * np.log(1 - y_pred + 1e-9)
        return -np.mean(y_zero_loss + y_one_loss)

    def compute_gradients(self, x, y_true, y_pred):
        # derivative of binary cross entropy
        difference =  y_pred - y_true
        gradient_b = np.mean(difference)
        gradients_w = np.matmul(x.transpose(), difference)
        gradients_w = np.array([np.mean(grad) for grad in gradients_w])

        return gradients_w, gradient_b

    def update_model_parameters(self, error_w, error_b):
        self.weights = self.weights - 0.1 * error_w
        self.bias = self.bias - 0.1 * error_b

    def predict(self, x):
        x_dot_weights = np.matmul(x, self.weights.transpose()) + self.bias
        probabilities = self._sigmoid(x_dot_weights)
        return [1 if p > 0.5 else 0 for p in probabilities]

    def _sigmoid(self, x):
        return np.array([self._sigmoid_function(value) for value in x])

    def _sigmoid_function(self, x):
        if x >= 0:
            z = np.exp(-x)
            return 1 / (1 + z)
        else:
            z = np.exp(x)
            return z / (1 + z)

    def _transform_x(self, x):
        x = copy.deepcopy(x)
        return x.values

    def _transform_y(self, y):
        y = copy.deepcopy(y)
        return y.values.reshape(y.shape[0], 1)

"""Entrenamos el modelo con 120 epocas.

"""

epochs_est = 120

model = CustomLogisticRegression()

model.fitGradDesc(x_train, y_train, x_test, y_test, epochs = epochs_est)

"""Usamos las metricas que nos ofrece la libreria de scikit learn para medir la precision del modelo."""

from sklearn.metrics import accuracy_score

pred = model.predict(x_test)
accuracy = accuracy_score(y_test, pred)
print(accuracy)

"""Con estos datos de precision ahora podemos plotear las gráficas de Accuracy_train vs Accuracy_test"""

import matplotlib.pyplot as plt

plt.plot(model.train_accuracies, label = 'Accuracy_Train')
plt.plot(model.test_accuracies, label = 'Accuracy_Test')
plt.title('Precisión de Modelo con Gradiente Descendente')
plt.grid()
plt.legend()
plt.show()

"""# Punto 3 - Punto 2 con Optimizadores"""

input_dim = x_train.shape[1]
num_clases = 1
print(input_dim)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import plot_model

def model_Nadam():
  model = Sequential()
  model.add(Dense(10, input_dim = input_dim, activation='relu'))
  model.add(Dense(10, activation='relu'))
  model.add(Dense(num_clases, activation='linear'))

  model.summary()

  opt = tf.keras.optimizers.Nadam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name="Nadam"
)
 
  model.compile(loss = 'mse', optimizer = opt ,metrics=['accuracy'])

  return model

def model_Adam():
  model = Sequential()
  model.add(Dense(10, input_dim = input_dim, activation='relu'))
  model.add(Dense(10, activation='relu'))
  model.add(Dense(num_clases, activation='linear'))

  model.summary()

  opt = tf.keras.optimizers.Adam(learning_rate=0.1)
 
  model.compile(loss = 'mse', optimizer = opt, metrics=['accuracy'])

  return model

def model_Adamax():
  model = Sequential()
  model.add(Dense(10, input_dim = input_dim, activation='relu'))
  model.add(Dense(num_clases, activation='linear'))

  model.summary()

  opt =tf.keras.optimizers.Adamax(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name="Adamax"
)
 
  model.compile(loss = 'mse', optimizer = opt, metrics=['accuracy'])

  return model

modelA1_Adam = model_Adam()

modelA2_Nadam = model_Nadam()

modelA2_Adamax = model_Adamax()

history1 = modelA1_Adam.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=250, batch_size=100, verbose=0)

history2 = modelA2_Nadam.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=250, batch_size=100, verbose=0)

history3 = modelA2_Adamax.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=250, batch_size=100, verbose=0)

"""Usamos las metricas que nos ofrece la libreria de scikit learn para medir la precision del modelo."""

accuracy = history1.history['accuracy'][-1]
print("La precisión final del modelo Adam es:", accuracy)

accuracy_2 = history2.history['accuracy'][-1]
print("La precisión final del modelo Nadam  es:", accuracy_2)

accuracy_3 = history3.history['accuracy'][-1]
print("La precisión final del modelo Adamax es:", accuracy_3)

"""Se puede observar que el accuracy mejor dado es el Nadam con 0.92 y comparado con el anterior modelo del punto 2 , es mejor usar este optimizador

Con estos datos de precision ahora podemos plotear las gráficas de Accuracy_train vs Accuracy_test
"""

import matplotlib.pyplot as plt

train_acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']

plt.plot(train_acc, label='Precisión de Entrenamiento')
plt.plot(val_acc, label='Precisión de Validación')
plt.title('Precisión del Modelo con Optimizador Adam')
plt.xlabel('Épocas')
plt.ylabel('Precisión')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

train_acc_2 = history2.history['accuracy']
val_acc_2 = history2.history['val_accuracy']

plt.plot(train_acc_2, label='Precisión de Entrenamiento')
plt.plot(val_acc_2, label='Precisión de Validación')
plt.title('Precisión del Modelo con Optimizador Nadam')
plt.xlabel('Épocas')
plt.ylabel('Precisión')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

train_acc_3 = history3.history['accuracy']
val_acc_3 = history3.history['val_accuracy']

plt.plot(train_acc_3, label='Precisión de Entrenamiento')
plt.plot(val_acc_3, label='Precisión de Validación')
plt.title('Precisión del Modelo con Optimizador Adamax')
plt.xlabel('Épocas')
plt.ylabel('Precisión')
plt.legend()
plt.show()

"""## Segunda parte punto 2 (Logistic Regression Sklearn)

Ahora se construirá un modelo usando la libreria sklearn mas especificamente la la herramienta "LogisticRegression". Una vez entrenado este nuevo modelo, se compararán los niveles de precision entre este nuevo y el anterior creado desde 0.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

model2 = LogisticRegression(solver = 'liblinear', max_iter = epochs_est)
model2.fit(x_train, y_train)
pred_model2 = model2.predict(x_test)
accuracy_model2 = accuracy_score(y_test, pred_model2)
print(accuracy_model2)

"""## Matrices de Confusión

Para comparar los dos modelos, se crearon matrices de confusion para cada modelo, con estas se puede realizar un mejor analisis de los resultados de los modelos.
"""

import itertools
def plot_confusion_matrix(cm, num_classes, opt, normalize=False,cmap=plt.cm.BuGn):      
      classes = ['Negative', 'Positive']

      plt.figure(figsize = (5,5))
      plt.imshow(cm, interpolation='nearest', cmap=cmap)
      plt.title('Confusion Matrix ' + opt)
      plt.colorbar()
      tick_marks = np.arange(len(classes))
      plt.xticks(tick_marks, classes, rotation=90)
      plt.yticks(tick_marks, classes)
      if normalize:
          cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
          cm = np.round(cm,2)
      thresh = cm.max() / 2.
      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
          plt.text(j, i, cm[i, j],
                   horizontalalignment="center",
                   color="white" if cm[i, j] > thresh else "black")
      plt.tight_layout()
      plt.ylabel('Original')
      plt.xlabel('Predictions')

"""### Confusion Matrix GD"""

from sklearn import metrics

print("Confusion Matrix GD \n")

conf_mat_1 = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(conf_mat_1, 2, 'GD')
print(metrics.classification_report(y_test, pred))

"""Según lo dado por la matriz, se tiene que en los datos de testeo, se clasifican bien los datos positivos, sin embargo, de los datos negativos se clasifican mal 2 de los 10 datos, pero para el dataset utilizado, se requiere que en datos negativos no haya personas clasificadas erroneamente, pues una salida predicted como 0 y en realidad es 1, en este contexto podria costarle la vida al paciente.

### Confusion Matrix Sklearn logistic regression model
"""

print("Matriz Confusión con Sklearn \n")
conf_mat_2 = metrics.confusion_matrix(y_test, pred_model2)
plot_confusion_matrix(conf_mat_2, 2, 'Sklearn')
print(metrics.classification_report(y_test, pred_model2))

"""Los dos modelos tienen un comportamiento similar, sin embargo, debido a la cantidad de epocas que se definieron es mejor el desempeño del modelo creado desde 0 que el dado por la libreria Sklearn, lo más probable es que su comportamineto mejore y supere al nuestro si se aumenta el número de epocas, o si se usa un solver diferente.

# Punto 3 Tabla

![tablapunto3.PNG](data:image/png;base64,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)

Cuando se dice que un optimizador como Adam **no tiene restricciones, significa que no impone ninguna limitación en los valores que pueden tomar los parámetros del modelo** durante el proceso de optimización. Esto significa que los valores de los parámetros pueden variar libremente dentro de su rango permitido y no se aplican restricciones adicionales durante la optimización

Por otro lado, cuando se dice que **un optimizador tiene mínimos locales, significa que su capacidad para encontrar el mínimo global de una función es limitada**. En cambio, el optimizador puede converger a un mínimo local, que es el mínimo más cercano al punto de partida del algoritmo de optimización.

En general, por ejemplo espera que Adam encuentre un mínimo local cercano a su punto de inicio, pero no necesariamente el mínimo global de la función de pérdida.

En la optimización matemática y en el aprendizaje automático, el término "mínimo" se refiere al valor más bajo de una función de pérdida o costo que se puede alcanzar mediante la variación de los parámetros del modelo. **El objetivo del proceso de optimización es encontrar los valores de los parámetros que minimizan la función de pérdida y, por lo tanto, maximizan el rendimiento del modelo**. Sin embargo, una función de pérdida puede tener varios mínimos diferentes.

**Un mínimo local es el valor más bajo de la función de pérdida en una región local de los parámetros**. En otras palabras, **es el punto más bajo que se puede alcanzar a partir de un conjunto particular de valores de parámetros**, pero no necesariamente el valor más bajo posible en todo el espacio de parámetros. Por otro lado, el mínimo global es el valor más bajo de la función de pérdida en todo el espacio de parámetros. Este es el punto más bajo que se puede alcanzar en la función de pérdida, independientemente de los valores iniciales de los parámetros o el camino seguido por el algoritmo de optimización

*En resumen*, **el mínimo local es el punto más bajo de la función de pérdida en una región local de los parámetros**, mientras que **el mínimo global es el punto más bajo de la función de pérdida en todo el espacio de parámetros**. La capacidad de un optimizador para encontrar el mínimo global es importante porque es lo que asegura que el modelo tenga el mejor rendimiento posible en términos de la función de pérdida.

Links de referencia punto 3:
* Adam
  * https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam
  * https://keras.io/api/optimizers/adam/
  * https://arxiv.org/pdf/1609.04747.pdf
* Nadam
  * https://arxiv.org/pdf/1412.6980.pdf
  * https://interactivechaos.com/es/manual/tutorial-de-machine-learning/gradient-descent
  * http://cs229.stanford.edu/proj2015/054_report.pdf
  * https://machinelearningmastery.com/gradient-descent-optimization-with-nadam-from-scratch/

* Adamax
  * https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/Adamax
  * https://machinelearningmastery.com/gradient-descent-optimization-with-adamax-from-scratch/

# Punto 4

Se decidió trabajar con el [dataset heart disease](https://www.kaggle.com/datasets/yasserh/heart-disease-dataset) ya este tipo de enfermedades es la primera causa de muerte en el mundo  [(17.5 millones de fallecimientos al año)](https://https://www.amgen.es/enfermedades-tratadas/hipercolesterolemia/enfermedad-cardiovascular-esp#:~:text=La%20enfermedad%20cardiovascular%20es%20la,29%25%20de%20los%20decesos%20totales.). El diagnostico temprano de estas enfermedades extiende la expectativa de vida de los pacientes que las padecen.

Info Dataset: 76 atributos (de los cuales se usa un subconjunto de 14, que es al que se refieren los experimentos publicados), 1 salida (tiene o no una enfermedad cardiaca)

## Modelo 1

## Imports
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import itertools

dataset_url='https://raw.githubusercontent.com/LuisFernandoPareja/HeartDataSet/main/heart.csv'
pd.set_option('display.max_columns', None)
dataset = pd.read_csv(dataset_url)
dataset.head(5)

"""## Normalization
Definimos una función para poder normalizar los valores del dataset entre dos valores escogidos (en este caso normalizaremos entre 0 y 1).
"""

def normalizar(x, xmax, xmin, ymax, ymin):
  m = (ymax - ymin) / (xmax - xmin)
  b = ymin - m*xmin

  y = m*x+b

  return y

"""Se calculan los maximos y minimos de cada entrada y de la salida, para normalizar el dataset entre 0 y 1, se usa la normalización entre 0 y 1."""

max = np.max(dataset).values
min = np.min(dataset).values
data = normalizar(dataset,max,min,1,0)
data.head(5)

"""## Split the dataset

Se separa el dataset en entradas (x) y salidas (y), tomando en cuenta que la columna llamada target es la salida del dataset. Se le realiza un reshape a las salidas para que tenga forma (num, 1). Posteriormente se hace la separación del dataset un 80% del dataset es para el entrenamiento y el otro 20% restante es para el test.
"""

x = np.array(data.drop(['target'], axis = 1))
y = np.array(data['target']).reshape(-1,1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1)

"""## Modelo
Empezamos definiendo las funciones sigmoid y relu junto a sus derivadas, ambas funciones las usaremos 
"""

# Avoid division by zero
epsilon = np.finfo(float).eps
 
# Sigmoid function and its differentiation
def sigmoid(z):
    return 1/(1+np.exp(-z.clip(-500, 500)))
def dsigmoid(z):
    s = sigmoid(z)
    return 2 * s * (1-s)
 
# ReLU function and its differentiation
def relu(z):
    return np.maximum(0, z)
def drelu(z):
    return (z > 0).astype(float)

"""Se crea la función de perdida (binary cross entropy), ya que nuestro problema es de clasificacion binaria."""

def cross_entropy(y, yhat):
    """Binary cross entropy function
        L = - y log yhat - (1-y) log (1-yhat)
    """
    return -(y.T @ np.log(yhat.clip(epsilon)) + (1-y.T) @ np.log((1-yhat).clip(epsilon))) / y.shape[1]
 
def d_cross_entropy(y, yhat):
    """ dL/dyhat """
    return - np.divide(y, yhat.clip(epsilon)) + np.divide(1-y, (1-yhat).clip(epsilon))

"""### Arquitectura del modelo
Se define la clase mlp, para crear una red multicapa de perceptron, donde se configura la red, en primer lugar se definen los parametros z, w, b y a para realizar los pasos hacia adelante una vez se tiene sus gradientes (dz, dw, db y da), que se calculan con la backpropagation, luego se definen algunas funciones para el calculo de los parametros:

* La función initialize se encargada de dar pesos y bias aleatorios al modelo inicial.

* La función forward calcula la salida del sistema, tomando cada entrada (a) y realizando la multiplicación por los pesos y la suma del bias (z = W a + b), e introduciendola en la función de activación, realizando la función en toda la red, siendo la salida de la red realmente su única capa.

* backward se encarga de realizar la propagación hacia atras, despues de cada paso hacia adelante, se calculan las gradientes del los pesos y de los bias, comenzando en la capa de salida hasta la capa de entrada.

* Tomando que el da de la última capa es la derivada de la perdida entre el yreal y yhat, se van calculando las gradientes de los parametros, yendo hacia la capa de entrada.

* Por último, la función update, se encarga de actualizar cada peso con su gradiente, usando la regla de actualización de la gradiente descendente, donde eta es la taza de aprendizaje (learning rate), que no es más que restarle al paramétro actual el producto de eta por la gradiente del mismo:
"""

class mlp:
    '''Multilayer perceptron using numpy
    '''
    def __init__(self, layersizes, activations, derivatives, lossderiv):
        """remember config, then initialize array to hold NN parameters without init"""
        # hold NN config
        self.layersizes = layersizes
        self.activations = activations
        self.derivatives = derivatives
        self.lossderiv = lossderiv
        # parameters, each is a 2D numpy array
        L = len(self.layersizes)
        self.z = [None] * L
        self.W = [None] * L
        self.b = [None] * L
        self.a = [None] * L
        self.dz = [None] * L
        self.dW = [None] * L
        self.db = [None] * L
        self.da = [None] * L
 
    def initialize(self, seed=42):
        np.random.seed(seed)
        sigma = 0.1
        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):
            self.W[l] = np.random.randn(insize, outsize) * sigma
            self.b[l] = np.random.randn(1, outsize) * sigma
 
    def forward(self, x):
        self.a[0] = x
        for l, func in enumerate(self.activations, 1):
            # z = W a + b, with `a` as output from previous layer
            # `W` is of size rxs and `a` the size sxn with n the number of data instances, `z` the size rxn
            # `b` is rx1 and broadcast to each column of `z`
            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]
            # a = g(z), with `a` as output of this layer, of size rxn
            self.a[l] = func(self.z[l])
        return self.a[-1]

    def backward(self, y, yhat):
        # first `da`, at the output
        self.da[-1] = self.lossderiv(y, yhat)
        for l, func in reversed(list(enumerate(self.derivatives, 1))):
            # compute the differentials at this layer
            self.dz[l] = self.da[l] * func(self.z[l])
            self.dW[l] = self.a[l-1].T @ self.dz[l]
            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)
            self.da[l-1] = self.dz[l] @ self.W[l].T
 
    def update(self, eta):
        for l in range(1, len(self.W)):
            self.W[l] -= eta * self.dW[l]
            self.b[l] -= eta * self.db[l]

"""Se define la estructura del modelo (las capas, funciones de activacion, funcion de perdida). Se calcula la pérdida actual del modelo, con los pesos y bias iniciales aleatorios, y se calcula la precisión inicial del modelo."""

from sklearn.metrics import accuracy_score
# Build a model
model = mlp(layersizes=[x.shape[1], 4, 3, 1],
            activations=[relu, relu, sigmoid],
            derivatives=[drelu, drelu, dsigmoid],
            lossderiv=d_cross_entropy)
model.initialize()
yhat = model.forward(x)
loss = cross_entropy(y, yhat)
print("Before training - loss value {} accuracy {}".format(loss, accuracy_score(y, (yhat > 0.5))))

"""Se definen los hiperparametros del modelo (epocas, learning rate, etc)."""

# train for each epoch
n_epochs = 150
learning_rate = 0.005
loss1 = np.zeros(n_epochs)
accuracy1 = np.zeros(n_epochs)
loss_test1 = np.zeros(n_epochs)
accuracy_test1 = np.zeros(n_epochs)

for n in range(n_epochs):

    #Calculo perdida y precisión testeo
    model.forward(x_test)
    yhat_t = model.a[-1]
    
    loss_test1[n]=cross_entropy(y_test, yhat_t)
    accuracy_test1[n] = accuracy_score(y_test, (yhat_t > 0.5))

    #Calculo perdida y precisión entrenamiento
    model.forward(x_train)
    yhat = model.a[-1]

    accuracy1[n] = accuracy_score(y_train, (yhat > 0.5))
    loss1[n]=cross_entropy(y_train, yhat)

    #Calculo backpropagation y actualización de los pesos, tomando en base medidas de entrenamiento
    model.backward(y_train, yhat)
    model.update(learning_rate)
    loss = cross_entropy(y_train, yhat)
    print("Iteration {} - loss value {} accuracy {}".format(n, loss, accuracy_score(y_train, (yhat > 0.5))))

"""Gráfica de perdida y precision"""

import matplotlib.pyplot as plt

plt.plot(loss1, label = 'Loss_train')
plt.plot(loss_test1, label = 'Loss_test')
plt.legend()
plt.grid()
plt.show()

plt.plot(accuracy1, label = 'Accuracy_train')
plt.plot(accuracy_test1, label = 'Accuracy_test')
plt.legend()
plt.grid()
plt.show()

print('Scores: ')
print("Training set score: {:.3f}".format(accuracy1[-1]))
print("Test set score: {:.3f}".format(accuracy_test1[-1]))

"""## Modelo 2 (MLP Sklearn)

Con ayuda de la libreria Sklearn, se implementa una red neuronal para clasificación, para esto se intenta tener los mismos hiperparámetros del modelo creado a mano, para ser justos en su comparación, por lo que se cambia su numero de capas ocultas, y de neuronas en la mismas, su número de epocas, su función de activación se usa la relu y para la de salida logisica (sigmoid) y tambien su optimizador, pues el de defecto es adam, que mucho más potente qu la gradiente descendente (opt modelo creado), por lo que se pone la gradiente descendente estocaica, que dentro de las opciones (‘lbfgs’, ‘sgd’, ‘adam’), se considera la más justa, para hacer la más parecida al gradiente descendiente, se usa un batch_size grande, igual a la cantidad de datos de entrenamiento (61).

Luego se entrena con los datos de entrenamiento y se calcula el score, que según la documentación de la libreria, es el la precisión promedia.
"""

from sklearn.neural_network import MLPClassifier

clf = MLPClassifier(random_state=1, max_iter=n_epochs,
                    hidden_layer_sizes=[4,3], 
                    solver = 'sgd',
                    batch_size = 61,
                    activation = 'relu')
clf.out_activation_ = 'logistic'
clf.fit(x_train, y_train)
print('Scores for MLP')
print("Training set score: {:.3f}".format(clf.score(x_train, y_train)))
print("Test set score: {:.3f}".format(clf.score(x_test, y_test)))

"""## Matrices de confusión

Para comparar los dos modelos, se crearon matrices de confusion para cada modelo, con estas se puede realizar un mejor analisis de los resultados de los modelos.
"""

def plot_confusion_matrix(cm, num_classes, opt, normalize=False,cmap=plt.cm.Reds):
      
      import itertools
      
      classes = ['Negativo', 'Positivo']

      plt.figure(figsize = (5,5))
      plt.imshow(cm, interpolation='nearest', cmap=cmap)
      plt.title('Matriz de Confusión ' + opt)
      plt.colorbar()
      tick_marks = np.arange(len(classes))
      plt.xticks(tick_marks, classes, rotation=90)
      plt.yticks(tick_marks, classes)
      if normalize:
          cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
          cm = np.round(cm,2)
      thresh = cm.max() / 2.
      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
          plt.text(j, i, cm[i, j],
                   horizontalalignment="center",
                   color="white" if cm[i, j] > thresh else "black")
      plt.tight_layout()
      plt.ylabel('Real')
      plt.xlabel('Predicciones')

"""## Confusion Matrix Modelo 1"""

from sklearn import metrics

print("Matriz Confusión Modelo 1 \n")

yhat1 = model.forward(x_test)

conf_mat_1 = metrics.confusion_matrix(y_test, (yhat1 > 0.5))
plot_confusion_matrix(conf_mat_1, 2, 'Modelo 1')
print(metrics.classification_report(y_test, (yhat1 > 0.5)))

"""## Confusion Matrix Modelo 2 (MLP Sklearn)"""

print("Matriz Confusión Modelo Sklearn \n")

yhat_sk = clf.predict(x_test)

conf_mat_sk = metrics.confusion_matrix(y_test, (yhat_sk > 0.5))
plot_confusion_matrix(conf_mat_sk, 2, 'Modelo 2 Sklearn')
print(metrics.classification_report(y_test, (yhat_sk > 0.5)))

"""En cuanto al desempeño del primer modelo se obtiene que cuenta con un número bajo de falsos negativos, sin embargo, aunque sea bajo, lo ideal es que sea 0 para que no sea mortal en los resultados equivocados.En cuento a los falsos positivos, se tiene una gran cantidad, pero estos solo infunden en la cantidad de examenes médicos que se van a tener que llevar a cabo para descartar.

Ahora observando el segundo modelo de Sklearn aunque cuente con una precisión muy baja, no tendría un riesgo grande, pues no hay falsos negativos, sin embargo, tampoco tendría mayor utilidad, pues pasa a todas las personas como que tienen enfermedades cardiacas, por l oque se tendria que realizar examenes a todos.

El aumento de neuronas y capas en los problemas de clasificación, puede mejorar el rendimiento de la red, permitiendo un análisis más profundo por parte del modelo, sin embargo, el pasarse con las mismas y el número de epócas puede provacar un sobre entrenamiento, donde solo quedarían bien clasificados los datos de entrenamiento y la red no serviria para recibir datos nuevos, por lo que sería obsoleta. Aunque la pérdida y precisión son buenas medidas para problemas de clasificación, el uso de la matriz de confusión nos permite ver mejor si el modelo sirve o no para la aplicación que lo estamos usando, para lo cual tambien es de vital importancia entender correctamente nuestro dataset, para tomar decisiones sobre que requerimos minizar, si los falsos positivos o los falsos negativos.

# Punto 5

**Data Set** :[Student](https://archive.ics.uci.edu/ml/datasets/Student+Performance)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from scipy.linalg import svd
from sklearn.decomposition import PCA

#leer un archivo CSV desde una URL y crear un DataFrame de Pandas que contiene los datos del archivo CSV
df = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/Students_Alcohol_Consumption/student-mat.csv')

#carga un conjunto de datos de estudiantes y alcohol desde una URL y crea un DataFrame de Pandas
url = "https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/Students_Alcohol_Consumption/student-mat.csv"
pd.set_option('display.max_columns', None)
dataset = pd.read_csv(url)
dataset.head(5)

#Asisgnar DataFrame a las variables X y Y
X = df[['age', 'Medu', 'Fedu', 'traveltime', 'studytime']]
y = df['G3']

# Dividir los datos en un conjunto de entrenamiento y un conjunto de prueba
# La variable X contiene las características y la variable y contiene la variable de respuesta
# test_size = 0.3 indica que el 30% de los datos se utilizarán para el conjunto de prueba
# random_state se establece en 0 para que los resultados sean reproducible

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Crear un modelo de regresión lineal y ajustarlo al conjunto de entrenamiento
reg = LinearRegression().fit(X_train, y_train)

# Predecir los valores de y para el conjunto de prueba utilizando el modelo ajustado
y_pred = reg.predict(X_test)

# Calcular el RMSE entre los valores reales y los valores predichos
rmse_orig = np.sqrt(mean_squared_error(y_test, y_pred))
print('RMSE con todas las variables de entrada:', rmse_orig)

#SVD reduccion 2
# Realizar la descomposición en valores singulares (SVD) de los datos originales
U, s, VT = svd(X)
# Utilizar las dos primeras componentes principales para reconstruir los datos originales 
X_2 = U[:, :2] @ np.diag(s[:2]) @ VT[:2, :]
# Dividir los datos reducidos en un conjunto de entrenamiento y un conjunto de prueba
X_train_2, X_test_2, y_train, y_test = train_test_split(X_2, y, test_size=0.3, random_state=0)
# Crear un modelo de regresión lineal y ajustarlo al conjunto de entrenamiento
reg_2 = LinearRegression().fit(X_train_2, y_train)
# Predecir los valores de y para el conjunto de prueba utilizando el modelo ajustado
y_pred_2 = reg_2.predict(X_test_2)

# Calcular el RMSE entre los valores reales y los valores predichos
rmse_2 = np.sqrt(mean_squared_error(y_test, y_pred_2))
print('RMSE con 2 variables de entrada:', rmse_2)

#PCA reduccion 2

# Realizar una reducción de dimensionalidad de los datos utilizando PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# Dividir los datos reducidos en un conjunto de entrenamiento y un conjunto de prueba
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=0)
# Ajustar un modelo de regresión lineal a los datos reducidos
reg = LinearRegression().fit(X_train, y_train)
# Predecir los valores de y para el conjunto de prueba utilizando el modelo ajustado
y_pred = reg.predict(X_test)

# Calcular el RMSE entre los valores reales y los valores predichos
rmse_pca_2 = np.sqrt(mean_squared_error(y_test, y_pred))
print('RMSE con 2 variables de entrada:', rmse_pca_2)

#SVD reduccion 1

# Utilizar la primera componente para reconstruir los datos originales
X_1 = U[:, :1] @ np.diag(s[:1]) @ VT[:1, :]
# Dividir los datos reducidos en un conjunto de entrenamiento y un conjunto de prueba
X_train_1, X_test_1, y_train, y_test = train_test_split(X_1, y, test_size=0.3, random_state=0)
# Crear un modelo de regresión lineal y ajustarlo al conjunto de entrenamiento
reg_1 = LinearRegression().fit(X_train_1, y_train)
# Predecir los valores de y para el conjunto de prueba utilizando el modelo ajustado
y_pred_1 = reg_1.predict(X_test_1)

# Calcular el RMSE entre los valores reales y los valores predichos
rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_1))
print('RMSE con 1 variable de entrada:', rmse_1)

#PCA reduccion 1

# Realizar una reducción de dimensionalidad de los datos utilizando PCA
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X)
# Dividir los datos reducidos en un conjunto de entrenamiento y un conjunto de prueba
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=0)
# Ajustar un modelo de regresión lineal a los datos reducidos
reg = LinearRegression().fit(X_train, y_train)
# Predecir los valores de y para el conjunto de prueba utilizando el modelo ajustado
y_pred = reg.predict(X_test)

# Calcular el RMSE entre los valores reales y los valores predichos
rmse_pca_1 = np.sqrt(mean_squared_error(y_test, y_pred))
print('RMSE con 1 variables de entrada:', rmse_pca_1)

print('RMSE con todas las variables de entrada:', rmse_orig)
print('----------------------------------------')
print('RMSE - SVD con 2 variables de entrada:', rmse_2)
print('RMSE - SVD con 1 variable de entrada: ', rmse_1)
print('----------------------------------------')
print('RMSE - PCA con 2 variables de entrada:', rmse_pca_2)
print('RMSE - PCA con 1 variable de entrada:', rmse_pca_1)